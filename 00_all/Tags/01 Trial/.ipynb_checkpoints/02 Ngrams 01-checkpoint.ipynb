{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "925e37a6",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e660b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fastDamerauLevenshtein\n",
    "#!pip install fuzzywuzzy\n",
    "#!pip install fuzzymatcher\n",
    "#!pip install sparse_dot_topn \n",
    "#!pip install editdistance\n",
    "#!pip install fastDamerauLevenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e49c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9be6a9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfec84e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process, fuzz\n",
    "#from fastDamerauLevenshtein import damerauLevenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b15eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "from difflib import SequenceMatcher, get_close_matches\n",
    "\n",
    "\n",
    "\n",
    "#if-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import sparse_dot_topn.sparse_dot_topn as ct\n",
    "from nltk.util import ngrams\n",
    "#import nltk.util.ngrams\n",
    "\n",
    "#Kavya\n",
    "import editdistance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66d6bed",
   "metadata": {},
   "source": [
    "# 1 Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a42916",
   "metadata": {},
   "source": [
    "## Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f12898",
   "metadata": {},
   "source": [
    "### 1.1 Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c24333e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('Aliasing_Master.xlsx') #Fix mention sheet name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fbbefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Aliasing_Master.csv',skipinitialspace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ade3bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading csv file and at a same time using converters attribute which will remove extra space\n",
    "df = pd.read_csv('Aliasing_Master.csv', converters={'PID': str.strip(),\n",
    "                                                'SPI' : str.strip(),\n",
    "                                                'PDS' : str.strip()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc3f400",
   "metadata": {},
   "source": [
    "### 1.2 Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360584ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6413f44",
   "metadata": {},
   "source": [
    "The features have different syntaxes across columns. White Spaces, seperators like '-' are present in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214f4927",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90d79d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Unnamed: 3',\t'Alias',\t'Master'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e374fd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca9ea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c835b4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8ed9ef",
   "metadata": {},
   "source": [
    "'PID', 'SPI', 'PDS' columns have Unique items and there are no duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760370a2",
   "metadata": {},
   "source": [
    "### 1.3 Data Pre-processing - Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21621636",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    df[i] = df[i].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5528b4e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (14299) does not match length of index (56420)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17228\\2960377115.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mnumeric_words_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumeric_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf'{feat}_bag_of_all'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf'{feat}_bag_of_alpha'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malpha_words_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf'{feat}_bag_of_num'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumeric_words_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3653\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3654\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3655\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3657\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3830\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3831\u001b[0m         \"\"\"\n\u001b[1;32m-> 3832\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3833\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3834\u001b[0m         if (\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4537\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4538\u001b[1;33m             \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4539\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \"\"\"\n\u001b[0;32m    556\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    558\u001b[0m             \u001b[1;34m\"Length of values \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[1;34mf\"({len(data)}) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (14299) does not match length of index (56420)"
     ]
    }
   ],
   "source": [
    "for feat in df.columns:\n",
    "    words_list = []\n",
    "    alpha_words_list = []\n",
    "    numeric_words_list = []\n",
    "    \n",
    "    for x in df[feat]:\n",
    "        if pd.notnull(x):\n",
    "            words = re.findall(r'[A-Za-z]+|\\d+', str(x))\n",
    "            alpha_words = [word for word in words if word.isalpha()]\n",
    "            numeric_words = [word for word in words if word.isdigit()]\n",
    "            \n",
    "            words_list.append(words)\n",
    "            alpha_words_list.append(alpha_words)\n",
    "            numeric_words_list.append(numeric_words)\n",
    "    \n",
    "    df[f'{feat}_bag_of_all'] = words_list\n",
    "    df[f'{feat}_bag_of_alpha'] = alpha_words_list\n",
    "    df[f'{feat}_bag_of_num'] = numeric_words_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da95cea7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17228\\3717703739.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Check if not null and not empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'[A-Za-z]+|\\d+'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0malpha_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "for feat in df.columns:\n",
    "    words_list = []\n",
    "    alpha_words_list = []\n",
    "    numeric_words_list = []\n",
    "\n",
    "    for x in df[feat]:\n",
    "        if len(x) > 0:  # Check if not null and not empty\n",
    "            words = re.findall(r'[A-Za-z]+|\\d+', str(x))\n",
    "            alpha_words = [word for word in words if word.isalpha()]\n",
    "            numeric_words = [word for word in words if word.isdigit()]\n",
    "\n",
    "            words_list.append(words)\n",
    "            alpha_words_list.append(alpha_words)\n",
    "            numeric_words_list.append(numeric_words)\n",
    "\n",
    "    if len(words_list) > 0:\n",
    "        df[f'{feat}_bag_of_all'] = pd.Series(words_list, index=df.index[:len(words_list)])\n",
    "        df[f'{feat}_bag_of_alpha'] = pd.Series(alpha_words_list, index=df.index[:len(alpha_words_list)])\n",
    "        df[f'{feat}_bag_of_num'] = pd.Series(numeric_words_list, index=df.index[:len(numeric_words_list)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac3dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2c6f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7754de1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spid_unique_df = pd.DataFrame()\n",
    "spi_unique_df = pd.DataFrame()\n",
    "pds_unique_df = pd.DataFrame()\n",
    "\n",
    "for i in df.columns:\n",
    "\n",
    "    #print('{0} Column Count is {1}'.format(i, df[i].count()))\n",
    "\n",
    "    if i == 'PID':\n",
    "        spid_unique_list = df['PID'].unique().tolist()\n",
    "        spid_unique_df[i] = spid_unique_list\n",
    "    elif i == 'SPI':\n",
    "        spi_unique_list = df['SPI'].unique().tolist()\n",
    "        spi_unique_df[i] = spi_unique_list\n",
    "    elif i == 'PDS':\n",
    "        pds_unique_list = df['PDS'].unique().tolist()\n",
    "        pds_unique_df[i] = pds_unique_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0f7b92",
   "metadata": {},
   "source": [
    "### 1.4 Unique DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a087e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df= pd.DataFrame()\n",
    "unique_list=[]\n",
    "\n",
    "for i in df.columns:\n",
    "    \n",
    "    unique_list = df[i].unique().astype('str').tolist()\n",
    "    unique_temp_df = pd.DataFrame(unique_list, dtype= 'str',columns =[i])\n",
    "    unique_df=pd.concat([unique_df,unique_temp_df],axis=1)\n",
    "    #exec('df_{} = pd.DataFrame()'.format(i))\n",
    "    #exec('df_{}'.format(i)[i]) = pd.DataFrame(unique_list, columns =i)\n",
    "    \n",
    "    #print('{0} Column Count is {2}. Unique Tags are: {1}'.format(i,len(unique_list)-1, df[i].count()))\n",
    "    #print(' ')\n",
    "          \n",
    "#print(unique_df.dtypes)\n",
    "#unique_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e9a24",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Unique Dataset shape:', unique_df.shape)\n",
    "unique_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d51ed5",
   "metadata": {},
   "source": [
    "### 1.5 Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6797d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"This is a string with whitespace and special characters !@#$%^&*()_+\"\n",
    "clean_text = re.sub(r'\\W+', '', str(text))\n",
    "\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9084ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df = unique_df.replace('\\s+', ' ', regex=True)\n",
    "# apply the function to each element in the dataframe\n",
    "\n",
    "unique_df = unique_df.replace('-', ' ', regex=True)\n",
    "\n",
    "# print the cleaned dataframe\n",
    "unique_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42177bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define a function to remove whitespace and special characters from a string\n",
    "#def clean_string(s):\n",
    "    #return re.sub(r'\\W+', '', str(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd14eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply the function to each element in the dataframe\n",
    "#df = df.applymap(clean_string)\n",
    "\n",
    "## print the cleaned dataframe\n",
    "#df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70fd110",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Future Scope, collect the '-' Index/position in each string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca48111",
   "metadata": {},
   "source": [
    "### 1.6 Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f204d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in df.columns:\n",
    " #   df[i].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6d5082",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d018cd53",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 1.7 Sort Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0bf87a",
   "metadata": {},
   "source": [
    "### 1.8 Small Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920f2ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spid_unique = unique_df['PID'].unique().tolist()\n",
    "spi_unique = unique_df['SPI'].unique().tolist()\n",
    "pds_unique = unique_df['PDS'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0375113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spid_small=spid_unique[:3000]\n",
    "spi_small=spi_unique[:3000]\n",
    "pds_small=pds_unique[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382ec2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.5 Future Scope - Bucketing/Binning into Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9c2d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.5 Future Scope - 2 Dimensional Array of Alphabet & Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaea5054",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 2 Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a414996b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as hc\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400b7ff1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a02c3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Convert texts to a matrix of TF-IDF features\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa88845",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Create the document-term matrix\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4cd3bb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 2.1 K-Means Clustering: 'PID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35de3846",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(spid_unique_df['PID'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b708fba",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "wss =[] \n",
    "for i in range(1,20):\n",
    "    KM = KMeans(n_clusters=i)\n",
    "    KM.fit(X)\n",
    "    wss.append(KM.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d356753",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "plt.plot(range(1,20), wss, marker='o', markerfacecolor='red', markersize=10)\n",
    "plt.xlabel('K (Number of Clusters)')\n",
    "plt.ylabel('Inertia Score')\n",
    "plt.title('Inertia vs K Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c573bd",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cluster the texts using K-Means algorithm\n",
    "kmeans = KMeans(n_clusters=8)\n",
    "kmeans.fit(X)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "spid_unique_df['PID_kmeans'] = labels\n",
    "\n",
    "# Print the cluster labels assigned to each text\n",
    "#for i, pid in enumerate(spid_unique):\n",
    " #   print(f\"Text {i+1} is assigned to cluster {kmeans.labels_[i]+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b32bac4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 2.2 Hierarchial Clustering: 'PID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce46e8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create the document-term matrix\n",
    "\n",
    "dtm = vectorizer.fit_transform(spid_unique_df['PID'].values.astype('U'))\n",
    "# Perform hierarchical clustering\n",
    "Z = linkage(dtm.toarray(), method='ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2069f55f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a dendrogram\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(Z, leaf_rotation=90., leaf_font_size=8.)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc63809",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#wardlink = linkage(dtm.toarray(), method = 'ward')\n",
    "#Show last p  merges, setting p-10, Fix- metric\n",
    "dend = dendrogram(Z, truncate_mode='lastp', p=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4520f5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Assign cluster to records using fcluster\n",
    "#Method1\n",
    "clusters = fcluster(Z, 23, criterion='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa324599",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spid_unique_df['PID_clusters'] = clusters\n",
    "spid_unique_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb691da",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spid_unique_df.to_csv('pid_clust.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8900c102",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 2.3 Hierarchial Clustering : 'SPI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078e80e0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create the document-term matrix\n",
    "\n",
    "dtm = vectorizer.fit_transform(spi_unique_df['SPI'].values.astype('U'))\n",
    "# Perform hierarchical clustering\n",
    "Z = linkage(dtm.toarray(), method='ward')\n",
    "\n",
    "# Create a dendrogram\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(Z, leaf_rotation=90., leaf_font_size=8.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee2bd0f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Each branch in the 'SPI' clusters are clean and 7 is a good cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e01c40f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Assign cluster to records using fcluster\n",
    "#Method1\n",
    "clusters = fcluster(Z, 7, criterion='maxclust')\n",
    "\n",
    "spi_unique_df['SPI_clusters'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7775e00e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spi_unique_df.to_csv('spi_clusters.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722cb2a4",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2.5 KMeans Clustering : 'PDS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb29309c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(pds_unique_df['PDS'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851272eb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Cluster the texts using K-Means algorithm\n",
    "\n",
    "wss =[] \n",
    "for i in range(1,20):\n",
    "    KM = KMeans(n_clusters=i, random_state=123)\n",
    "    KM.fit(X)\n",
    "    wss.append(KM.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba82294",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "plt.plot(range(1,20), wss, marker='o', markerfacecolor='red', markersize=10)\n",
    "plt.xlabel('K (Number of Clusters)')\n",
    "plt.ylabel('Inertia Score')\n",
    "plt.title('Inertia vs K Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c175ce",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Cluster the texts using K-Means algorithm\n",
    "kmeans = KMeans(n_clusters=10, random_state=123)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d93759c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "labels = kmeans.labels_\n",
    "pds_unique_df['PDS_kmeans'] = labels\n",
    "\n",
    "pds_unique_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3e6b31",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2.6 Hierrachial Clustering : 'PDS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53913fc5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create the document-term matrix\n",
    "\n",
    "dtm = vectorizer.fit_transform(pds_unique_df['PDS'].values.astype('U'))\n",
    "# Perform hierarchical clustering\n",
    "Z = linkage(dtm.toarray(), method='ward')\n",
    "\n",
    "# Create a dendrogram\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(Z, leaf_rotation=90., leaf_font_size=8.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbc46ae",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Each branch in the 'PDS' clusters are clean and 7 is a good cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5228366",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Assign cluster to records using fcluster\n",
    "#Method1\n",
    "clusters = fcluster(Z, 7, criterion='maxclust')\n",
    "\n",
    "pds_unique_df['PDS_clusters'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae13b983",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pds_unique_df.to_csv('pds_clusters.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1179f7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Assumption now observation after SPID and Instr Bidirectional test:\n",
    "\n",
    "PID is master left column.\n",
    "\n",
    "Direction of the search is PID searched in SPI, PID is serached in PDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6743ae",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Best Match for PDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa556af",
   "metadata": {},
   "source": [
    "# 2 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9ea9fb",
   "metadata": {},
   "source": [
    "## 2.1 FuzzyWuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c559245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query model\n",
    "#results = process.extract(query, choices, scorer=fuzz.token_sort_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee85b0a",
   "metadata": {},
   "source": [
    "### 2.1.1 Algorithm 0 -FuzzyWuzzy  Instr Tag in SPID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5147ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create tuples of brand names, matched brand names, and the score\n",
    "score_sort = [(x,) + i\n",
    "             for x in spi_small\n",
    "             for i in process.extract(x, spid_small, scorer=fuzz.token_sort_ratio)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b9af4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe from the tuples\n",
    "similarity_sort = pd.DataFrame(score_sort, columns=['spi_sort','spid_sort','score_sort'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274cf35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff= 81\n",
    "similarity_sort['similarity_flag'] = (similarity_sort['score_sort'] > cutoff).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60a8d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x in spid column.\n",
    "similarity_sort.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fe8541",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_sort.to_csv('Fuzzy_10k_Instr_in_SPID.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245f3789",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_sort.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055de397",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 2.1.2 Algorithm 1- FuzzyWuzzy SPID Tag in Instr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba6aeef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Create tuples of brand names, matched brand names, and the score\n",
    "score_sort = [(x,) + i\n",
    "             for x in spid_small\n",
    "             for i in process.extract(x, spi_small, scorer=fuzz.token_sort_ratio)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d3a23b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Create dataframe from the tuples\n",
    "similarity_sort = pd.DataFrame(score_sort, columns=['spid_sort','spi_sort','score_sort'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969c33e7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cutoff= 80\n",
    "similarity_sort['similarity_flag'] = (similarity_sort['score_sort'] >= cutoff).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684dd5b7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# x in Instr column.\n",
    "similarity_sort.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0e4ef9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Derive representative values\n",
    "similarity_sort['sorted_brand_sort'] = np.minimum(similarity_sort['spi_sort'], similarity_sort['spid_sort'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d5b9a6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "similarity_sort.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaf6396",
   "metadata": {},
   "source": [
    "### 2.1.3 SPID in PDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e09daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create tuples of brand names, matched brand names, and the score\n",
    "#(spid_unique_df['PID'].values.astype('U')\n",
    "score_sort = [(x,) + i\n",
    "             for x in unique_df['PDS'].values.astype('U')\n",
    "             for i in process.extract(x, spid_small, scorer=fuzz.token_sort_ratio)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89562ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe from the tuples\n",
    "similarity_sort = pd.DataFrame(score_sort, columns=[ 'pds_sort', 'spid_sort','score_sort'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27d9467",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff= 81\n",
    "similarity_sort['similarity_flag'] = (similarity_sort['score_sort'] > cutoff).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413a9c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_sort.to_csv('Fuzzy_10k_PDS_in_SPID.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394c508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_sort.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be92b43c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 2.2 Algorithm 1- Knuth Morris - Sivaji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5a0574",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 2.3 Algorithm 2- Boyer-Moore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a628187",
   "metadata": {
    "hidden": true
   },
   "source": [
    "https://cmps-people.ok.ubc.ca/ylucet/DS/BoyerMoore.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be83014a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "https://www.cs.utexas.edu/users/moore/best-ideas/string-searching/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270f393a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "https://www.geeksforgeeks.org/boyer-moore-algorithm-for-pattern-searching/````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a253777a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 2.4 Algorithm 3- Damerau Levenstein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d6077e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "similarity:\n",
    "\n",
    "If this parameter value is False, it will return the total cost of edit, otherwise it will return a score from 0.0 to 1.0 denoting how similar the two objects are. It is True by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2858fc87",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fastDamerauLevenshtein import damerauLevenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72745e51",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "damerauLevenshtein('ca', 'abc', similarity=False)  # expected result: 2.0\n",
    "damerauLevenshtein('car', 'cars', similarity=True)  # expected result: 0.75\n",
    "damerauLevenshtein(['ab', 'bc'], ['ab'], similarity=False)  # expected result: 1.0\n",
    "damerauLevenshtein(['ab', 'bc'], ['ab'], similarity=True)  # expected result: 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8bd8a2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/damerau-levenshtein-distance/\n",
    "    \n",
    "def optimal_string_alignment_distance(s1, s2):\n",
    "    # Create a table to store the results of subproblems\n",
    "    dp = [[0 for j in range(len(s2)+1)] for i in range(len(s1)+1)]\n",
    "     \n",
    "    # Initialize the table\n",
    "    for i in range(len(s1)+1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len(s2)+1):\n",
    "        dp[0][j] = j\n",
    " \n",
    "    # Populate the table using dynamic programming\n",
    "    for i in range(1, len(s1)+1):\n",
    "        for j in range(1, len(s2)+1):\n",
    "            if s1[i-1] == s2[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])\n",
    " \n",
    "    # Return the edit distance\n",
    "    return dp[len(s1)][len(s2)]\n",
    " \n",
    "print(optimal_string_alignment_distance(\"geeks\", \"forgeeks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fffd55",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x= 'JE-CRC301-FE-80'\n",
    "y = 'JE-CRC301-FE-801'\n",
    "lev_dist = optimal_string_alignment_distance(x, y)\n",
    "lev_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8b7613",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Create tuples of brand names, matched brand names, and the score\n",
    "for x in spid_small:\n",
    "    for y in spi_small:\n",
    "        print(optimal_string_alignment_distance(x, y))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db17245",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#https://pypi.org/project/editdistance/\n",
    "\n",
    "import editdistance\n",
    "editdistance.eval('banana', 'bahama')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c847c2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The challenge is that these algorithms (e.g. Levenshtein, Damerau-Levenshtein, Jaro-Winkler, q-gram, cosine) are computationally intensive. Trying to do a lot of matching on large data sets is not scaleable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d3e3a0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 2.5 Entity Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82ad853",
   "metadata": {
    "hidden": true
   },
   "source": [
    "https://www.districtdatalabs.com/basics-of-entity-resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5957e54",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 2.6 tf-idf \n",
    "\n",
    "https://towardsdatascience.com/fuzzy-matching-at-scale-84f2bfd0c536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e42cc1c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bc5a5d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from scipy.sparse import csr_matrix\n",
    "#!pip install sparse_dot_topn \n",
    "#import sparse_dot_topn.sparse_dot_topn as ct\n",
    "\n",
    "org_names = df['PID'].unique()\n",
    "vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams)\n",
    "tf_idf_matrix = vectorizer.fit_transform(org_names)\n",
    "\n",
    "def awesome_cossim_top(A, B, ntop, lower_bound=0):\n",
    "    # force A and B as a CSR matrix.\n",
    "    # If they have already been CSR, there is no overhead\n",
    "    A = A.tocsr()\n",
    "    B = B.tocsr()\n",
    "    M, _ = A.shape\n",
    "    _, N = B.shape\n",
    " \n",
    "    idx_dtype = np.int32\n",
    " \n",
    "    nnz_max = M*ntop\n",
    " \n",
    "    indptr = np.zeros(M+1, dtype=idx_dtype)\n",
    "    indices = np.zeros(nnz_max, dtype=idx_dtype)\n",
    "    data = np.zeros(nnz_max, dtype=A.dtype)\n",
    "ct.sparse_dot_topn(\n",
    "        M, N, np.asarray(A.indptr, dtype=idx_dtype),\n",
    "        np.asarray(A.indices, dtype=idx_dtype),\n",
    "        A.data,\n",
    "        np.asarray(B.indptr, dtype=idx_dtype),\n",
    "        np.asarray(B.indices, dtype=idx_dtype),\n",
    "        B.data,\n",
    "        ntop,\n",
    "        lower_bound,\n",
    "        indptr, indices, data)\n",
    "return csr_matrix((data,indices,indptr),shape=(M,N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a425b5",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 2.7 Kavya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdc2edb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import editdistance\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils import get_column_letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8517300",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbd5a40",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from Levenshtein import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2d0c66",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel('Aliasing_Master.xlsx')\n",
    "column1 = data['PID'].astype(str).tolist()\n",
    "column2 = data['SPI'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661c7843",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "threshold = 3\n",
    "matches = []\n",
    "for i in range(len(column1)):\n",
    "    for j in range(len(column2)):\n",
    "        if editdistance.distance(column1[i], column2[j]) <= threshold:\n",
    "            matches.append((i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b482ff4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!pip install editdistance\n",
    "\n",
    "import editdistance\n",
    "\n",
    "str1 = \"example\"\n",
    "str2 = \"samples\"\n",
    "\n",
    "distance = editdistance.damerau_levenshtein_distance(str1, str2)\n",
    "print(distance)  # Output: 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd391ca",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "for i, j in matches:\n",
    "    ws[f'A{i+1}'] = column1[i]\n",
    "    ws[f'B{i+1}'] = column2[j]\n",
    "\n",
    " \n",
    "\n",
    "wb.save('matched_pairs.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2facc5b",
   "metadata": {},
   "source": [
    "## 2.8 Fast Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d61cd6b",
   "metadata": {},
   "source": [
    "https://github.com/nahida-uap/Text-Similarity-Metrics-in-Python/blob/master/fastText-embedding-model-soft-cosine-similarity.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007689c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import require libraries\n",
    "import gensim\n",
    "from gensim.matutils import softcossim \n",
    "from gensim import corpora\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "#print(gensim.__version__)\n",
    "\n",
    "# Download the FastText model\n",
    "fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')\n",
    "\n",
    "#Define the documents\n",
    "rev1 = \"Overall the app is very good. I always use it. But after the Android 10 update on the Google pixel 2 xl, I'm not able to chat with other contacts while I'm on video call with picture to picture mode like before. Emailed support and it's like they didn't even try to understand what the problem was and just replied with random links resolutions. Not sure if it was Android update issue or what. Still have the issue.\"\n",
    "\n",
    "rev2 = \"This would be a much better app if it wasn't such a pain in the arse to update contacts. How hard does it have to be? *Edit*. A year on, and still a pain. Trying to get a contact who is on WhatsApp into the contact list just doesn't happen. You can refresh until you're blye in the face. It still wont add them.\"\n",
    "\n",
    "rev3 = \"A Feature request: so my whatsapp media is not saved to my phone gallery because most of them are memes , greetings,news but sometimes it's Important photos/videos which i want to save in gallery and there is no option to save this photo to gallery . Please add this feature it would be very helpfull\"\n",
    "\n",
    "rev4 = \"The what's new is a lie. It's said the same thing for multiple updates. Why the secretive updates? What are you doing to our devices? // 20 Aug another secretive update with false changelog // 3 Sep *another* update delivered with a FALSE update. This is unaceptable. This is digital rape.\"\n",
    "\n",
    "rev5 = \"Hii..... WhatsApp My model is motog4 plus ....not show fingerprint lock option why please answer dedo bhai in Hindi????\"\n",
    "\n",
    "rev6 = \"We need to have improvements on the quality phone conversations. No smooth flow of conversation, and too many reconnecting instances in one chat, or Tel conversation.\"\n",
    "\n",
    "documents = [rev1, rev2, rev3, rev4, rev5, rev6]\n",
    "\n",
    "\n",
    "#Prepare a dictionary and a corpus.\n",
    "dictionary = corpora.Dictionary([simple_preprocess(doc) for doc in documents])\n",
    "\n",
    "#Prepare the similarity matrix\n",
    "similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n",
    "\n",
    "#Embeddings - Convert the sentences into bag-of-words vectors.\n",
    "sent_1 = dictionary.doc2bow(simple_preprocess(rev1))\n",
    "sent_2 = dictionary.doc2bow(simple_preprocess(rev2))\n",
    "sent_3 = dictionary.doc2bow(simple_preprocess(rev3))\n",
    "sent_4 = dictionary.doc2bow(simple_preprocess(rev4))\n",
    "sent_5 = dictionary.doc2bow(simple_preprocess(rev5))\n",
    "sent_6 = dictionary.doc2bow(simple_preprocess(rev6))\n",
    "\n",
    "sentences = [sent_1, sent_2, sent_3, sent_4, sent_5, sent_6]\n",
    "\n",
    "\n",
    "#Compute soft cosine similarity of 2 documents\n",
    "#print(softcossim(sent_1, sent_2, similarity_matrix))\n",
    "\n",
    "#Compute soft cosine similarity matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def soft_cosine_similarity_matrix(sentences):\n",
    "    len_array = np.arange(len(sentences))\n",
    "    xx, yy = np.meshgrid(len_array, len_array)\n",
    "    cossim_mat = pd.DataFrame([[round(softcossim(sentences[i],sentences[j], similarity_matrix) ,2) for i, j in zip(x,y)] for y, x in zip(xx, yy)])\n",
    "    return cossim_mat\n",
    "\n",
    "#print(documents)\n",
    "soft_cosine_similarity_matrix(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08ba34c",
   "metadata": {},
   "source": [
    "# 3 Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46386a33",
   "metadata": {},
   "source": [
    "## 3.1 TDIFDVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fd7c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf79346d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the two strings to compare\n",
    "s1 = \"C301FE-150007\"\n",
    "s2 = 'JE-CRC301-FE-150007'\n",
    "\n",
    "# Create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the documents and transform the documents into TF-IDF vectors\n",
    "vectors = vectorizer.fit_transform([s1, s2])\n",
    "\n",
    "# Compute the cosine similarity between the two vectors\n",
    "similarity = cosine_similarity(vectors[0], vectors[1])[0][0]\n",
    "\n",
    "print(\"The similarity score between the two strings is:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d651caec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(spid_unique_df['PID'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b300c3e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "condition = similarity_sort['similarity_flag'] == 1\n",
    "filtered_df = similarity_sort[condition]\n",
    "print(filtered_df.shape)\n",
    "filtered_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45134dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.drop(['score_sort','similarity_flag'],axis=1).values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56919e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d99a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df= pd.DataFrame(filtered_df, columns=['spi_sort', 'spid_sort'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30ba184",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf712045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the cosine similarity matrix\n",
    "#spid_unique_df['PID'].values.astype('U')\n",
    "cosine_sim = cosine_similarity(eval_df.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c6455",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(cosine_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb60ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s1 in spid_unique:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fed89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To use cosine similarity on two string features in a dataframe, you can follow these steps:\n",
    "\n",
    "#Load the necessary libraries: pandas, numpy, and scikit-learn.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#Create a pandas dataframe with the two string features you want to compare. For example:\n",
    "\n",
    "data = similarity_score\n",
    "df = pd.DataFrame(data)\n",
    "#Instantiate a CountVectorizer object to transform the string data into a bag-of-words model, where each word is represented by a feature in a vector. This will be used to calculate the cosine similarity.\n",
    "\n",
    "vectorizer = CountVectorizer().fit_transform(df['string1'] + df['string2'])\n",
    "#Split the vectorized data into two separate sets for the two string features you want to compare.\n",
    "\n",
    "vectorizer1 = vectorizer[:len(df)]\n",
    "vectorizer2 = vectorizer[len(df):]\n",
    "#Calculate the cosine similarity matrix between the two sets of vectorized data using cosine_similarity function from scikit-learn.\n",
    "\n",
    "similarity_matrix = cosine_similarity(vectorizer1, vectorizer2)\n",
    "#Store the cosine similarity matrix in a new dataframe with rows labeled with the indices of string1 and columns labeled with the indices of string2.\n",
    "\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=df.index, columns=df.index)\n",
    "#Display the resulting similarity dataframe to compare the two string features.\n",
    "\n",
    "print(similarity_df)\n",
    "#This will output a square dataframe where each cell represents the cosine similarity between the string data at the corresponding row and column indices.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b482a2",
   "metadata": {},
   "source": [
    "## 3.2 Fuzzy Score Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3c90f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62d17455",
   "metadata": {},
   "source": [
    "## 3.3 Jaccard Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727fc094",
   "metadata": {},
   "source": [
    "### 3.3.1 Jaccard Scores between SPID & SPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf7c4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35927f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "j_df= pd.read_csv('Fuzzy_10k_Instr_in_SPID.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5ad6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "j_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fc2886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def jaccard_similarity(col1, col2):\n",
    "    set1 = set(col1.split())\n",
    "    set2 = set(col2.split())\n",
    "    return len(set1.intersection(set2)) / float(len(set1.union(set2)))\n",
    "\n",
    "j_df['jaccard_score'] = j_df.apply(lambda row: jaccard_similarity(row['spi_sort'], row['spid_sort']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cd75e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "j_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99662da4",
   "metadata": {},
   "source": [
    "Observation: The Jaccard Score and combination of fuzzy score can be used to identify bets match."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0266c4",
   "metadata": {},
   "source": [
    "### 3.3.1 Jaccard Scores between SPID & PDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376cb40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pds_j_df= pd.read_csv('Fuzzy_10k_PDS_in_SPID.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144c9d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "pds_j_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa1b5e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def jaccard_similarity(col1, col2):\n",
    "    set1 = set(col1.values.astype('U').split())\n",
    "    set2 = set(col2.values.astype('U').split())\n",
    "    return len(set1.intersection(set2)) / float(len(set1.union(set2)))\n",
    "\n",
    "pds_j_df['jaccard_score'] = pds_j_df.apply(lambda row: jaccard_similarity(row['pds_sort'], row['spid_sort']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8fad2c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pds_j_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a4b0f4",
   "metadata": {},
   "source": [
    "## 3.4 Human Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a32515",
   "metadata": {},
   "source": [
    "### 3.4.1 Labeling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fc672b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "719bc675",
   "metadata": {},
   "source": [
    "### 3.4.2 Confidence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225a331b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelled data is needed as output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e823ca5c",
   "metadata": {},
   "source": [
    "# 4 Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0383cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf42342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "matches = awesome_cossim_top(tf_idf_matrix, tf_idf_matrix.transpose(), 10, 0.85)\n",
    "t = time.time()-t1\n",
    "print(\"SELFTIMED:\", t)\n",
    "Outputs:\n",
    "  SELFTIMED: 0.19s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90e3163",
   "metadata": {},
   "outputs": [],
   "source": [
    "Example Remark:\n",
    "    \n",
    "    'E-101' is '70%' similar to 'C-FE-102' after '2' 'Alphabet' Insertion(s) & '1' 'Numeric' Modification(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d57788",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 5 Business Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69942a6f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f9cd312",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 6 Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d48a83",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90211522",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 6.2 Data Pre-processing Unique DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a722aa25",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    print('{0} Column Count is {1}:'.format(i,df[i].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bcb606",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17a7780c",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 1.6 Convert astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cddb44",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data[\"PID\"].apply(lambda x: str(x))\n",
    "data['PID'].dtypes\n",
    "\n",
    "data=data.astype(str)\n",
    "\n",
    "for i in data.columns:\n",
    "    data[i].apply(str)\n",
    "    #data.astype({i:'string'})\n",
    "    #data[i].values.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16af9afb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35e8483c",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 6.6 Data Pre-processing- Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e73ab38",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as hc\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "import scipy.cluster.hierarchy as hc\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "clust_pids = unique_df.drop(['SPI', 'PDS'], axis=1)\n",
    "\n",
    "clust_spis = unique_df.drop(['PID','PDS'], axis=1)\n",
    "\n",
    "clust_pdss = unique_df.drop(['SPI', 'PID'], axis=1)\n",
    "\n",
    "clust_pids.head(3)\n",
    "\n",
    "clust_pids.isnull().sum()\n",
    "\n",
    "type(clust_pids)\n",
    "\n",
    "clust_pids.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1decc9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Clustering ChatGPT\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The brown fox jumps over the brown dog.\",\n",
    "    \"The red fox jumps over the brown dog.\",\n",
    "    \"The brown cat jumps over the brown dog.\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "k = 2  # number of clusters\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "kmeans.fit(X)\n",
    "\n",
    "for i in range(k):\n",
    "    print(\"Cluster\", i+1, \":\")\n",
    "    for j in range(len(corpus)):\n",
    "        if kmeans.labels_[j] == i:\n",
    "            print(\"-\", corpus[j])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aa77e9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "https://vitalflux.com/hierarchical-clustering-explained-with-python-example/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aa2711",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/how-to-easily-cluster-textual-data-in-python-ab27040b07d8\n",
    "\n",
    "def cluster_text(text):\n",
    "    vectorizer = TfidfVectorizer(stop_words={'english'})\n",
    "    X = vectorizer.fit_transform(text)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.cluster import KMeans\n",
    "    Sum_of_squared_distances = []\n",
    "    K = range(2,10)\n",
    "    for k in K:\n",
    "       km = KMeans(n_clusters=k, max_iter=200, n_init=10)\n",
    "       km = km.fit(X)\n",
    "       Sum_of_squared_distances.append(km.inertia_)\n",
    "    plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Sum_of_squared_distances')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.show()\n",
    "\n",
    "    print('How many clusters do you want to use?')\n",
    "    true_k = int(input())\n",
    "    model = KMeans(n_clusters=true_k, init='k-means++', max_iter=200, n_init=10)\n",
    "    model.fit(X)\n",
    "\n",
    "    labels=model.labels_\n",
    "    clusters=pd.DataFrame(list(zip(text,labels)),columns=['title','cluster'])\n",
    "    #print(clusters.sort_values(by=['cluster']))\n",
    "\n",
    "    for i in range(true_k):\n",
    "        print(clusters[clusters['cluster'] == i])\n",
    "        \n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
