{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d11b445d",
   "metadata": {},
   "source": [
    "https://blog.floydhub.com/gentle-introduction-to-text-summarization-in-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd1c373",
   "metadata": {},
   "source": [
    "Here is the code blueprint of the summarizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eba603",
   "metadata": {},
   "source": [
    "Here are the steps for creating a simple text summarizer in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e680d3f0",
   "metadata": {},
   "source": [
    "### Step 1: Preparing the data\n",
    "\n",
    "In this example, we want to summarize the information found on (https://en.wikipedia.org/wiki/20th_century) Wikipedia article, which just gives an overview of the major happenings during the 20th century.\n",
    "\n",
    "To enable us to fetch the article’s text, we’ll use the Beautiful Soup library.\n",
    "\n",
    "Here is the code for scraping the article’s content:Here is the code for scraping the article’s content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4f3e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as BeautifulSoup\n",
    "import urllib.request  \n",
    "\n",
    "# Fetching the content from the URL\n",
    "fetched_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/20th_century')\n",
    "\n",
    "article_read = fetched_data.read()\n",
    "\n",
    "# Parsing the URL content and storing in a variable\n",
    "article_parsed = BeautifulSoup.BeautifulSoup(article_read,'html.parser')\n",
    "\n",
    "# Returning <p> tags\n",
    "paragraphs = article_parsed.find_all('p')\n",
    "\n",
    "article_content = ''\n",
    "\n",
    "# Looping through the paragraphs and adding them to the variable\n",
    "for p in paragraphs:  \n",
    "    article_content += p.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705e013f",
   "metadata": {},
   "source": [
    "In the above code, we begin by importing the essential libraries for fetching data from the web page. The BeautifulSoup library is used for parsing the page while the urllib library is used for connecting to the page and retrieving the HTML.\n",
    "\n",
    "BeautifulSoup converts the incoming text to Unicode characters and the outgoing text to UTF-8 characters, saving you the hassle of managing different charset encodings while scraping text from the web.\n",
    "\n",
    "We’ll use the urlopen function from the urllib.request utility to open the web page. Then, we’ll use the read function to read the scraped data object. For parsing the data, we’ll call the BeautifulSoup object and pass two parameters to it; that is, the article_read and the html.parser.\n",
    "\n",
    "The find_all function is used to return all the <p> elements present in the HTML. Furthermore, using .text enables us to select only the texts found within the <p> elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e829c26f",
   "metadata": {},
   "source": [
    "### Step 2: Processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499b099a",
   "metadata": {},
   "source": [
    "To ensure the scrapped textual data is as noise-free as possible, we’ll perform some basic text cleaning.  To assist us to do the processing, we’ll import a list of stopwords from the nltk library.\n",
    "\n",
    "We’ll also import PorterStemmer, which is an algorithm for reducing words into their root forms. For example, cleaning, cleaned, and cleaner can be reduced to the root clean.\n",
    "\n",
    "Furthermore, we’ll create a dictionary table having the frequency of occurrence of each of the words in the text. We’ll loop through the text and the corresponding words to eliminate any stop words.\n",
    "\n",
    "Then, we’ll check if the words are present in the frequency_table. If the word was previously available in the dictionary, its value is updated by 1. Otherwise, if the word is recognized for the first time, its value is set to 1.\n",
    "\n",
    "For example, the frequency table should look like the following:\n",
    "\n",
    "WORD\tFREQUENCY\n",
    "\n",
    "century\t7\n",
    "\n",
    "world\t4\n",
    "\n",
    "United States\t3\n",
    "\n",
    "computer\t1\n",
    "\n",
    "Here is the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9edb23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "def _create_dictionary_table(text_string) -> dict:\n",
    "   \n",
    "    # Removing stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    words = word_tokenize(text_string)\n",
    "    \n",
    "    # Reducing words to their root form\n",
    "    stem = PorterStemmer()\n",
    "    \n",
    "    # Creating dictionary for the word frequency table\n",
    "    frequency_table = dict()\n",
    "    for wd in words:\n",
    "        wd = stem.stem(wd)\n",
    "        if wd in stop_words:\n",
    "            continue\n",
    "        if wd in frequency_table:\n",
    "            frequency_table[wd] += 1\n",
    "        else:\n",
    "            frequency_table[wd] = 1\n",
    "\n",
    "    return frequency_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29d7450",
   "metadata": {},
   "source": [
    "### Step 3:  Tokenizing the article into sentences\n",
    "\n",
    "To split the article_content into a set of sentences, we’ll use the built-in method from the nltk library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e66fca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(article_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34fb4e5",
   "metadata": {},
   "source": [
    "### Step 4: Finding the weighted frequencies of the sentences\n",
    "\n",
    "To evaluate the score for every sentence in the text, we’ll be analyzing the frequency of occurrence of each term. In this case, we’ll be scoring each sentence by its words; that is, adding the frequency of each important word found in the sentence.\n",
    "\n",
    "Take a look at the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e8155a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_sentence_scores(sentences, frequency_table) -> dict:   \n",
    "\n",
    "    # Algorithm for scoring a sentence by its words\n",
    "    sentence_weight = dict()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_wordcount = (len(word_tokenize(sentence)))\n",
    "        sentence_wordcount_without_stop_words = 0\n",
    "        for word_weight in frequency_table:\n",
    "            if word_weight in sentence.lower():\n",
    "                sentence_wordcount_without_stop_words += 1\n",
    "                if sentence[:7] in sentence_weight:\n",
    "                    sentence_weight[sentence[:7]] += frequency_table[word_weight]\n",
    "                else:\n",
    "                    sentence_weight[sentence[:7]] = frequency_table[word_weight]\n",
    "\n",
    "        sentence_weight[sentence[:7]] = sentence_weight[sentence[:7]] /        sentence_wordcount_without_stop_words\n",
    "      \n",
    "    return sentence_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c7e501",
   "metadata": {},
   "source": [
    "Importantly, to ensure long sentences do not have unnecessarily high scores over short sentences, we divided each score of a sentence by the number of words found in that sentence.\n",
    "\n",
    "Also, to optimize the dictionary’s memory, we arbitrarily added sentence[:7], which refers to the first 7 characters in each sentence. However, for longer documents, where you are likely to encounter sentences with the same first n_chars, it’s better to use hash functions or smart index functions to take into account such edge-cases and avoid collisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1382e1e0",
   "metadata": {},
   "source": [
    "### Step 5: Calculating the threshold of the sentences\n",
    "\n",
    "To further tweak the kind of sentences eligible for summarization, we’ll create the average score for the sentences. With this threshold, we can avoid selecting the sentences with a lower score than the average score.\n",
    "\n",
    "Here is the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "285a93c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_average_score(sentence_weight) -> int:\n",
    "   \n",
    "    # Calculating the average score for the sentences\n",
    "    sum_values = 0\n",
    "    for entry in sentence_weight:\n",
    "        sum_values += sentence_weight[entry]\n",
    "\n",
    "    # Getting sentence average value from source text\n",
    "    average_score = (sum_values / len(sentence_weight))\n",
    "\n",
    "    return average_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e3978",
   "metadata": {},
   "source": [
    "### Step 6: Getting the summary\n",
    "\n",
    "Lastly, since we have all the required parameters, we can now generate a summary for the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3410481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_article_summary(sentences, sentence_weight, threshold):\n",
    "    sentence_counter = 0\n",
    "    article_summary = ''\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if sentence[:7] in sentence_weight and sentence_weight[sentence[:7]] >= (threshold):\n",
    "            article_summary += \" \" + sentence\n",
    "            sentence_counter += 1\n",
    "\n",
    "    return article_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e06570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_article_summary(article):\n",
    "    \n",
    "    #creating a dictionary for the word frequency table\n",
    "    frequency_table = _create_dictionary_table(article)\n",
    "\n",
    "    #tokenizing the sentences\n",
    "    sentences = sent_tokenize(article)\n",
    "\n",
    "    #algorithm for scoring a sentence by its words\n",
    "    sentence_scores = _calculate_sentence_scores(sentences, frequency_table)\n",
    "\n",
    "    #getting the threshold\n",
    "    threshold = _calculate_average_score(sentence_scores)\n",
    "\n",
    "    #producing the summary\n",
    "    article_summary = _get_article_summary(sentences, sentence_scores, 1.5 * threshold)\n",
    "\n",
    "    return article_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f34bd2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Humans explored space for the first time, taking their first footsteps on the Moon. However, these same wars resulted in the destruction of the imperial system. At the beginning of the period, the British Empire was the world's most powerful nation,[14] having acted as the world's policeman for the past century. The victorious Bolsheviks then established the Soviet Union, the world's first communist state. After the victory of the Allies in Europe, the war in Asia ended with the Soviet invasion of Manchuria and the dropping of two atomic bombs on Japan by the US, the first nation to develop nuclear weapons and the only one to use them in warfare. In total, World War II left some 60 million people dead. After the war, Germany was occupied and divided between the Western powers and the Soviet Union. With the Axis defeated and Britain and France rebuilding, the United States and the Soviet Union were left standing as the world's only superpowers. At the beginning of the century, strong discrimination based on race and sex was significant in most societies. With the end of colonialism and the Cold War, nearly a billion people in Africa were left in new nation states. The world was undergoing its second major period of globalization; the first, which started in the 18th century, having been terminated by World War I. Since the US was in a dominant position, a major part of the process was Americanization. Terrorism, dictatorship, and the spread of nuclear weapons were pressing global issues. The world was still blighted by small-scale wars and other violent conflicts, fueled by competition over resources and by ethnic conflicts. Millions were infected with HIV, the virus which causes AIDS. This includes deaths caused by wars, genocide, politicide and mass murders. Later in the 20th century, the development of computers led to the establishment of a theory of computation.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    summary_results = _run_article_summary(article_content)\n",
    "    print(summary_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a952e7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
