{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7096620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d86d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ({'User-Agent':\n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
    "            'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "URL = 'https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_1'\n",
    "#URL ='https://www.amazon.in/s?k=tools&crid=3QE50ZDWTVK46&sprefix=tool%2Caps%2C255&ref=nb_sb_noss_1'\n",
    "#URL ='https://www.amazon.in/gp/bestsellers/?ref_=nav_cs_bestsellers'\n",
    "webpage = requests.get(URL, headers=headers)\n",
    "soup = BeautifulSoup(webpage.content, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19231b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "#headers = {'User-Agent': 'Chrome/111.0.5563.65'}\n",
    "URL = 'https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_1'\n",
    "webpage = requests.get(URL, headers=headers)\n",
    "soup = BeautifulSoup(webpage.content, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b521e0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Outer Tag Object\n",
    "\n",
    "section_title= soup.find(\"span\", attrs={'class':'a-size-medium a-color-base a-text-normal'})\n",
    "print(section_title.text)\n",
    "print(type(section_title.text))\n",
    "print(type(section_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b08ce59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names=[] \n",
    "for d in soup.findAll(\"span\", attrs={'class':'a-size-medium a-color-base a-text-normal'}):\n",
    "    name = d.text\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a80448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "545cf192",
   "metadata": {},
   "source": [
    "Part 1:\n",
    "\n",
    "\n",
    "In this assignment you are required to scrape all products from this URL:\n",
    "https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_1\n",
    "\n",
    "Need to scrape atleast 20 pages of product listing pages\n",
    "Items to scrape:\n",
    "\n",
    "• Product URL - a class a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal\n",
    "\n",
    "• Product Name - span class a-size-medium a-color-base a-text-normal\n",
    "\n",
    "• Product Price span class - a-price-whole\n",
    "\n",
    "• Rating - span a-size-base \n",
    "\n",
    "• Number of reviews span class a-size-base s-underline-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4679003d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Outer Tag Objects\n",
    "i=0\n",
    "for d in soup.findAll(\"span\", attrs={'class':'a-size-medium a-color-base a-text-normal'}):\n",
    "    name = d.text\n",
    "    #rating = d.a-size-base\n",
    "    print(name)\n",
    "    prd_url = ('a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal')\n",
    "    i=1+i\n",
    "    print(i, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5bdbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.html import fromstring\n",
    "from lxml.html.clean import clean_html\n",
    "\n",
    "    # lxml\n",
    "\n",
    "    doc = fromstring(html_content)\n",
    "    text = clean_html(doc).text_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d4ce1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Outer Tag Objects\n",
    "#a-size-medium a-color-base a-text-normal\n",
    "#aok-relative\n",
    "#for d in soup.findAll('div', attrs={'class':'aok-relative'}):\n",
    "\n",
    "i=0\n",
    "for d in soup.findAll('div', attrs={'class':'aok-relative'}):\n",
    "    \n",
    "    href = d.find('a', attrs={'class':'a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal'})\n",
    "    # name = d.find('span', attrs={'class':'a-size-medium a-color-base a-text-normal'})\n",
    "    name = d.text\n",
    "    #name = d.find('span', attrs={'class':'zg-text-center-align'})\n",
    "    price= d.find('span', attrs={'class':'a-price-whole'})\n",
    "    rating = d.find('span', attrs={'class':'a-size-base'})\n",
    "    reviews_count = d.find('span', attrs={'class':'a-size-base s-underline-text'})\n",
    "    i=1+i\n",
    "    if name is not None:\n",
    "        print(i,   name, price, rating, reviews_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a52e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in soup.findAll('div', attrs={'class':'a-section a-spacing-none aok-relative'}):\n",
    "        #print(d)\n",
    "        name = d.find('span', attrs={'class':'zg-text-center-align'})\n",
    "        n = name.find_all('img', alt=True)\n",
    "        #print(n[0]['alt'])\n",
    "        author = d.find('a', attrs={'class':'a-size-small a-link-child'})\n",
    "        rating = d.find('span', attrs={'class':'a-icon-alt'})\n",
    "        users_rated = d.find('a', attrs={'class':'a-size-small a-link-normal'})\n",
    "        price = d.find('span', attrs={'class':'p13n-sc-price'})\n",
    "        print(name, author, rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec14c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alls = []\n",
    "hrefs=[]\n",
    "for d in soup.findAll('div', attrs={'class':'aok-relative'}):\n",
    "    alt_name = d.find('a', attrs={'class':'a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal'})\n",
    "    name = d.find('span', attrs={'class':'a-size-medium a-color-base a-text-normal'})\n",
    "    #name = d.text\n",
    "    #name = d.find('span', attrs={'class':'zg-text-center-align'})\n",
    "    href = soup.find(\"a\", attrs={'class':'a-link-normal s-underline-text s-underline-link-text s-link-style'})\n",
    "    price= d.find('span', attrs={'class':'a-price-whole'})\n",
    "    rating = d.find('span', attrs={'class':'a-size-base'})\n",
    "    reviews_count = d.find('span', attrs={'class':'a-size-base s-underline-text'})\n",
    "    #print(href.text)\n",
    "    #name = d.find('span', attrs={'class':'zg-text-center-align'})\n",
    "    #n = name.find_all('img', alt=True)\n",
    "    all1=[]\n",
    "    if href is not None:\n",
    "        all1.append(href.text)\n",
    "        #href.text prints extra trailing whitespace at end, fix needed.\n",
    "        hrefs.append(href.text)\n",
    "        #all1=[]\n",
    "    if alt_name is not None:\n",
    "        all1.append(alt_name.text)\n",
    "    else:\n",
    "        all1.append('Unknown Product Name')\n",
    "        \n",
    "    if name is not None:\n",
    "        all1.append(name.text)\n",
    "    else:\n",
    "        all1.append('Unknown Product Name')\n",
    "        \n",
    "    if price is not None:\n",
    "        all1.append(price.text)\n",
    "    else:\n",
    "        all1.append('N/A')\n",
    "        \n",
    "    if rating is not None:\n",
    "        all1.append(rating.text)\n",
    "    else:\n",
    "        all1.append('0')\n",
    "        \n",
    "    if reviews_count is not None:\n",
    "        all1.append(reviews_count.text)\n",
    "        alls.append(all1)\n",
    "alls\n",
    "\n",
    "csv_headers = ['url', 'Alt Product Name' , 'Product Name', 'Price', 'User_Rating', 'Reviews Count']\n",
    "df = pd.DataFrame(alls)\n",
    "df.columns=csv_headers\n",
    "df.head()\n",
    "#df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47407391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe8c504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2265e3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_data():  \n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "    url = 'https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_2'\n",
    "    r = requests.get(url, headers=headers)#, proxies=proxies)\n",
    "    content = r.content\n",
    "    soup = BeautifulSoup(content)\n",
    "    #print(soup)\n",
    "\n",
    "    rows = []\n",
    "    for d in soup.findAll('div', attrs={'class':'a-section a-spacing-none aok-relative'}):\n",
    "        print(d)\n",
    "        name = d.find('span', attrs={'class':'zg-text-center-align'})\n",
    "        n = name.find_all('img', alt=True)\n",
    "        rating = d.find('span', attrs={'class':'a-icon-alt'}) #<--- it's under <span> with class a-icon-alt, not a-link-normal\n",
    "        users_rated = d.find('a', attrs={'class':'a-size-small a-link-normal'})\n",
    "        price = d.find('span', attrs={'class':'p13n-sc-price'})\n",
    "        print(name.text)\n",
    "\n",
    "        row={}\n",
    "\n",
    "        if name is not None:\n",
    "            #print(n[0]['alt'])\n",
    "            row['Name'] = n[0]['alt']\n",
    "        else:\n",
    "            row['Name'] = \"unknown-product\"\n",
    "\n",
    "        if rating is not None:\n",
    "            #print(rating.text)\n",
    "            row['Rating'] = rating.text\n",
    "        else:\n",
    "            row['Rating'] = 'N/A'\n",
    "\n",
    "        if users_rated is not None:\n",
    "            #print(price.text)\n",
    "            row['Customers_Rated'] = users_rated.text\n",
    "        else:\n",
    "            row['Customers_Rated'] = '0'   \n",
    "\n",
    "        if price is not None:\n",
    "            #print(price.text)\n",
    "            row['Price'] = price.text\n",
    "        else:\n",
    "            row['Price'] = '0'\n",
    "        rows.append(row)    \n",
    "    return rows\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "results += get_data()\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('amazon_products.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3719ffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    for item in soup.find_all(\"span\", class_=\"a-profile-name\"):\n",
    "      data_string = data_string + item.get_text()\n",
    "      names.append(data_string)\n",
    "      data_string = \"\"  \n",
    "    \n",
    "    for item in soup.find_all(\"span\", {\"data-hook\": \"review-body\"}):\n",
    "      data_string = data_string + item.get_text()\n",
    "      reviews.append(data_string)\n",
    "      data_string = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5782383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152b4ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fca64f93",
   "metadata": {},
   "source": [
    "Part 2:\n",
    "\n",
    "With the Product URL received in the above case, hit each URL, and add below items:\n",
    "• Description\n",
    "\n",
    "• ASIN\n",
    "\n",
    "• Product Description\n",
    "\n",
    "• Manufacturer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b73854",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.amazon.com/s?k=books&i=stripbooks\"\n",
    "\n",
    "for i in range(1, 15):\n",
    "    temp = \"\"\"{\n",
    "    \"extract_rules\": {\n",
    "        \"Title\" : \"h2 > a > span\",\n",
    "        \"Price\" : \"a.a-size-base:nth-child(1) > span.a-price:nth-child(2) > span:nth-child(1)\"\n",
    "    },\n",
    "    \"wait\": 0,\n",
    "    \"url\": \"\"\"+\"\\\"\"+base_url + \"&page={0}\".format(i)+\"\\\"\"+\"\"\",\n",
    "    \"proxy_country\": \"US\",\n",
    "    \"proxy_type\": \"residential\"\n",
    "    }\"\"\"\n",
    "payload = (json.dumps(json.loads(temp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a6bfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # regex to extract required strings\n",
    "    tag = 'span class=\"a-size-medium a-color-base a-text-normal\"'\n",
    "    reg_str = \"<\" + tag + \">(.*?)</\" + tag + \">\"\n",
    "    res = re.findall(reg_str, name)\n",
    "    # printing result\n",
    "    print(\"The Strings extracted : \" + str(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbb9909",
   "metadata": {},
   "outputs": [],
   "source": [
    "alls=[]\n",
    "hrefs=[]\n",
    "for d in soup.findAll('div', attrs={'class':'aok-relative'}):\n",
    "    all1=[]\n",
    "    alt_name = d.find('a', attrs={'class':'a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal'})\n",
    "    #name = d.find('span', attrs={'class':'a-size-medium a-color-base a-text-normal'})\n",
    "    all1.append(alt_name.text)\n",
    "   # alls.append(all1)\n",
    "alls                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26edab94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The webpage URL\n",
    "headers = {'User-Agent': 'Mozilla/5.0',  'Accept-Language': 'en-US, en;q=0.5'}\n",
    "#headers = {'User-Agent': 'Chrome/111.0.5563.65'}\n",
    "URL = \"https://www.amazon.in/s?k=juice&crid=3BEU07G5USXIT&sprefix=juic%2Caps%2C704&ref=nb_sb_noss_2\"\n",
    "\n",
    "# HTTP Request\n",
    "\n",
    "webpage = requests.get(URL, headers=headers)\n",
    "# Soup Object containing all data\n",
    "soup = BeautifulSoup(webpage.content, \"lxml\")\n",
    "# Fetch links as List of Tag Objects\n",
    "links = soup.find_all(\"a\", attrs={'class':'a-link-normal s-no-outline'})\n",
    "# Store the links\n",
    "links_list = []\n",
    "# Loop for extracting links from Tag Objects\n",
    "\n",
    "for link in links:\n",
    "    links_list.append(link.get('href'))\n",
    "    # Loop for extracting product details from each link\n",
    "\n",
    "df2= pd.DataFrame(links_list)\n",
    "\n",
    "    \n",
    "for link in links_list:\n",
    "    new_webpage = requests.get(\"https://www.amazon.in\" + link, headers=headers)\n",
    "    new_soup = BeautifulSoup(new_webpage.content, \"lxml\")\n",
    "    # Function calls to display all necessary product information\n",
    "    print(\"Product Title =\", get_title(new_soup))\n",
    "    print(\"Product Price =\", extract_current_price(new_soup))\n",
    "    print(\"Product Rating =\", get_rating(new_soup))\n",
    "    print(\"Number of Product Reviews =\", get_review_count(new_soup))\n",
    "    #print(\"Availability =\", get_availability(new_soup))\n",
    "    print(\"Cost: \", get_cost(new_soup))\n",
    "    #print('link:' , link)\n",
    "    \n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39047c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "index_page = 'https://techinstr.myshopify.com/collections/all'\n",
    "url = 'https://techinstr.myshopify.com/collections/all?page={}'\n",
    "\n",
    "session = requests.session()\n",
    "response = session.get(index_page)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "count_element = soup.select_one('.filters-toolbar__product-count')\n",
    "count_str = count_element.text.replace('products', '')\n",
    "total_count = int(count_str)\n",
    "# Do more with the first page.\n",
    "\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0',  'Accept-Language': 'en-US, en;q=0.5'}\n",
    "#headers = {'User-Agent': 'Chrome/111.0.5563.65'}\n",
    "URL = 'https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_1'\n",
    "webpage = requests.get(URL, headers=headers)\n",
    "soup = BeautifulSoup(webpage.text, \"lxml\")\n",
    "\n",
    "\n",
    "results=[]\n",
    "page_count = math.ceil(total_count/8)\n",
    "for page_number in range(2, page_count+1):\n",
    "    res1=[]\n",
    "    response = session.get(url.format(page_number))\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    first_product = soup.select_one('.product-card:nth-child(1) > a > span')\n",
    "    #print(first_product.text.strip())\n",
    "    res1.append(first_product.text.strip())\n",
    "    results.append(res1)\n",
    "    # Do more with each of the pages.\n",
    "    \n",
    "df3= pd.DataFrame(results)\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3a2853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0',  'Accept-Language': 'en-US, en;q=0.5'}\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\",  'Accept-Language': 'en-US, en;q=0.5'}\n",
    "#headers = {'User-Agent': 'Chrome/111.0.5563.65'}\n",
    "URL = 'https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_1'\n",
    "URL = 'https://www.amazon.in/s?k=trolleys&crid=2F260ROT2TTOH&sprefix=trolle%2Caps%2C340&ref=nb_sb_noss_2'\n",
    "webpage = requests.get(URL, headers=headers)\n",
    "soup = BeautifulSoup(webpage.content, \"lxml\")\n",
    "\n",
    "\n",
    "links = soup.find_all(\"a\", attrs={'class':'a-link-normal s-no-outline'})\n",
    "# Store the links\n",
    "links_list = []\n",
    "# Loop for extracting links from Tag Objects\n",
    "for link in links:\n",
    "    links_list.append(link.get('href'))\n",
    "df_links=pd.DataFrame(links)\n",
    "print(df_links.head())\n",
    "alls=[]\n",
    "\n",
    "hrefs=[]\n",
    "for d in soup.findAll('div', attrs={'class':'aok-relative'}):\n",
    "    \n",
    "   \n",
    "    alt_name = d.find('a', attrs={'class':'a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal'})\n",
    "    name = d.find('span', attrs={'class':'a-size-medium a-color-base a-text-normal'})\n",
    "\n",
    "    price= d.find('span', attrs={'class':'a-price-whole'})\n",
    "    rating = d.find('span', attrs={'class':'a-size-base'})\n",
    "    reviews_count = d.find('span', attrs={'class':'a-size-base s-underline-text'})\n",
    "    \n",
    "    all1=[]\n",
    "\n",
    "    if alt_name is not None:\n",
    "        all1.append(alt_name.text)\n",
    "        #alt_name.text prints extra trailing whitespace at end, fix needed.\n",
    "    else:\n",
    "        all1.append('Unknown')\n",
    "        alls.append(all1)\n",
    "\n",
    "    \n",
    "df3= pd.DataFrame(alls)\n",
    "\n",
    "\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef8b81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url = 'https://www.amazon.in/s?k=mats&crid=32A9GTKX3AHV3&sprefix=mat%2Caps%2C229&ref=nb_sb_noss_1'\n",
    "\n",
    "# add header\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36'\n",
    "}\n",
    "r = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(r.content, \"lxml\")\n",
    "\n",
    "links_list=[]\n",
    "links = soup.find_all('a', {'class': 'a-link-normal s-no-outline'})\n",
    "\n",
    "for link in links:\n",
    "    link1=[]\n",
    "    #print(link.get('href'))\n",
    "    href_text = link.get('href')\n",
    "    link1.append(href_text)\n",
    "    links_list.append(link1)\n",
    "    \n",
    "links_df = pd.DataFrame(links_list)\n",
    "print(links_df.shape)\n",
    "links_df.head()\n",
    "\n",
    "# Function to extract Product Description / Feature\n",
    "def get_cost(soup):\n",
    "    try:\n",
    "        cost = soup.find('span', attrs={'class':'a-offscreen'}).string.strip()\n",
    "    except AttributeError:\n",
    "        cost=''\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "# Function to extract Product Title\n",
    "def get_title(soup):\n",
    "\t\n",
    "\ttry:\n",
    "\t\t# Outer Tag Object\n",
    "\t\ttitle = soup.find(\"span\", attrs={\"id\":'productTitle'})\n",
    "\n",
    "\t\t# Inner NavigatableString Object\n",
    "\t\ttitle_value = title.string\n",
    "\n",
    "\t\t# Title as a string value\n",
    "\t\ttitle_string = title_value.strip()\n",
    "\n",
    "\t\t# # Printing types of values for efficient understanding\n",
    "\t\t# print(type(title))\n",
    "\t\t# print(type(title_value))\n",
    "\t\t# print(type(title_string))\n",
    "\t\t# print()\n",
    "\n",
    "\texcept AttributeError:\n",
    "\t\ttitle_string = \"\"\t\n",
    "\n",
    "\treturn title_string\n",
    "\n",
    "\n",
    "# Function to extract Product Rating\n",
    "def get_rating(soup):\n",
    "\n",
    "\ttry:\n",
    "\t\trating = soup.find(\"i\", attrs={'class':'a-icon a-icon-star a-star-4-5'}).string.strip()\n",
    "\t\t\n",
    "\texcept AttributeError:\n",
    "\t\t\n",
    "\t\ttry:\n",
    "\t\t\trating = soup.find(\"span\", attrs={'class':'a-icon-alt'}).string.strip()\n",
    "\t\texcept:\n",
    "\t\t\trating = \"\"\t\n",
    "\n",
    "\treturn rating\n",
    "\n",
    "# Function to extract Number of User Reviews\n",
    "def get_review_count(soup):\n",
    "\ttry:\n",
    "\t\treview_count = soup.find(\"span\", attrs={'id':'acrCustomerReviewText'}).string.strip()\n",
    "\t\t\n",
    "\texcept AttributeError:\n",
    "\t\treview_count = \"\"\t\n",
    "\n",
    "\treturn review_count\n",
    "\n",
    "# Loop for extracting product details from each link \n",
    "for link in links_list:\n",
    "    new_webpage = requests.get(\"https://www.amazon.in\" + , headers=headers)\n",
    "    new_soup = BeautifulSoup(new_webpage.content, \"lxml\")\n",
    "    # Function calls to display all necessary product information\n",
    "    print(\"Product Title =\", get_title(new_soup))\n",
    "    print(\"Product Price =\", get_cost(new_soup))\n",
    "    print(\"Product Rating =\", get_rating(new_soup))\n",
    "    print(\"Number of Product Reviews =\", get_review_count(new_soup))\n",
    "    print(\"Availability =\", get_availability(new_soup))\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "links_df.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f154aaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url = 'https://www.amazon.in/s?k=mats&crid=32A9GTKX3AHV3&sprefix=mat%2Caps%2C229&ref=nb_sb_noss_1'\n",
    "\n",
    "# add header\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36'\n",
    "}\n",
    "r = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(r.content, \"lxml\")\n",
    "\n",
    "links_list=[]\n",
    "\n",
    "#print(soup.find_all(\"a\"))\n",
    "\n",
    "\n",
    "links = soup.find_all('a', {'class': 'a-link-normal s-no-outline'})\n",
    "\n",
    "for link in links:\n",
    "    link1=[]\n",
    "    #print(link.get('href'))\n",
    "    href_text = link.get('href')\n",
    "    link1.append(href_text)\n",
    "    links_list.append(link1)\n",
    "links_df = pd.DataFrame(links_list)\n",
    "print(links_df.shape)\n",
    "links_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e411fcb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
